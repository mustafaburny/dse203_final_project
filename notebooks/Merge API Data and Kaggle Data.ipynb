{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a347e0-9fb6-4a32-bb02-79d086197e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import glob\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d01043-7ab8-4025-9012-3d19fc49894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_CLIENT_ID = '4FEO91ch16g-P_n8AxU2_A'\n",
    "REDDIT_CLIENT_SECRET = 'Q2cyR53G5a6IpsSAzLbevNtBiITYhw'\n",
    "REDDIT_USER_AGENT = 'desktop:DSE203 (by u/Life_is_Life)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e493ca42-9071-43b9-8250-cf5642738d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id = REDDIT_CLIENT_ID,\n",
    "    client_secret = REDDIT_CLIENT_SECRET,\n",
    "    user_agent = REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f746a92-a2a8-446b-80f0-6bd2d3773136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be a decorator for disk caching of API calls\n",
    "def persist_to_file(file_name):\n",
    "    def decorator(original_func):\n",
    "\n",
    "        try:\n",
    "            cache = json.load(open(file_name, 'r'))\n",
    "        except (IOError, ValueError):\n",
    "            cache = {}\n",
    "\n",
    "        def new_func(param):\n",
    "            if param not in cache:\n",
    "                cache[param] = original_func(param)\n",
    "                json.dump(cache, open(file_name, 'w'))\n",
    "            return cache[param]\n",
    "\n",
    "        return new_func\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1167e1-330a-432c-b392-1833c3632104",
   "metadata": {},
   "source": [
    "# Get company search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd18b151-3993-4593-9a7a-6feb6725cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_regex_filter(stock_symbol):\n",
    "    companies_data = json.load(open('../dse203_final_project/companies.json'))\n",
    "    \n",
    "    search_q = list()\n",
    "    \n",
    "    search_q.append('$' + stock_symbol)\n",
    "    search_q.append(stock_symbol)\n",
    "    \n",
    "    for other_term in companies_data[stock_symbol]:\n",
    "        if ' ' in other_term:\n",
    "            search_q.append('\"' + other_term + '\"')\n",
    "        else:\n",
    "            search_q.append(other_term)\n",
    "    \n",
    "    return '|'.join(search_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f3d82-9fb4-4a21-824c-34425124575d",
   "metadata": {},
   "source": [
    "# Dataset 1: Reddit Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e092ccb-b077-4ede-b223-14db1f79dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_posts = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a9b24c0-8592-4aeb-ac7e-1dc63f16c4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6c56e1b6bd492fa55bb361f994bd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all posts from manually extracted dataset\n",
    "for json_file in tqdm_notebook(glob.glob('top_submissions_and_comments/top_submissions/*.json')):\n",
    "    # Extract stock symbol from file name\n",
    "    stock_symbol = os.path.basename(json_file)[12:-5]\n",
    "    \n",
    "    # Read in JSON file data\n",
    "    with open(json_file) as f:\n",
    "        json_file_data = json.load(f)\n",
    "    \n",
    "    for record in json_file_data:\n",
    "        reddit_posts.append({\n",
    "            'stock_symbol': stock_symbol,\n",
    "            'author': record['author'],\n",
    "            'created_utc': datetime.datetime.fromtimestamp(int(record['created_utc'])).isoformat(),\n",
    "            'id': record['id'],\n",
    "            'num_comments': record['num_comments'],\n",
    "            'num_crossposts': record['num_crossposts'],\n",
    "            'selftext': record.get('selftext'),\n",
    "            'subreddit': record['subreddit'],\n",
    "            'title': record['title'],\n",
    "            'url': record['url'],\n",
    "            'source': 'Custom Reddit API Query',\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a18f64-dbe4-4332-8f80-33712281bde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa9840136f549a2a6ea2f9b366c124a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve matching company posts from kaggle posts data\n",
    "@persist_to_file(os.path.join('all_data', 'submission_ids_without_author_info.json'))\n",
    "def get_submission_author(submission_id):\n",
    "    author = reddit.submission(id=submission_id).author\n",
    "    time.sleep(0.5)\n",
    "    if author is not None:\n",
    "        return author.name\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "companies_data = json.load(open('../dse203_final_project/companies.json'))\n",
    "kaggle_posts_df = pd.read_csv('kaggle_data/wsb-aug-2021-posts.csv')\n",
    "\n",
    "for stock_symbol in tqdm_notebook(companies_data):\n",
    "    selector = (\n",
    "        kaggle_posts_df['title'].str.match(construct_regex_filter(stock_symbol), flags=re.I) | \\\n",
    "        kaggle_posts_df['selftext'].str.match(construct_regex_filter(stock_symbol), flags=re.I)\n",
    "    )\n",
    "    for _, record in kaggle_posts_df[selector].iterrows():\n",
    "        reddit_posts.append({\n",
    "            'stock_symbol': stock_symbol,\n",
    "            'author': get_submission_author(record['id']),\n",
    "            'created_utc': datetime.datetime.fromtimestamp(int(record['created_utc'])).isoformat(),\n",
    "            'id': record['id'],\n",
    "            'num_comments': None,\n",
    "            'num_crossposts': None,\n",
    "            'selftext': record['selftext'],\n",
    "            'subreddit': 'wallstreetbets',\n",
    "            'title': record['title'],\n",
    "            'url': record['url'],\n",
    "            'source': 'Kaggle',\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a386e-6d5a-4f09-a6b2-ad98aa44a2e1",
   "metadata": {},
   "source": [
    "# Dataset 2: Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9a6ece9-84dc-494c-8780-0d7c5d981b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_comments = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52baa58b-ef43-4969-b53f-24df06510889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a34decdff7494caa0d49bd6aa85706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get all comments from manually extracted dataset\n",
    "for json_file in tqdm_notebook(glob.glob('top_submissions_and_comments/top_comments/*.json')):\n",
    "    # Extract stock symbol from file name\n",
    "    stock_symbol = os.path.basename(json_file)[12:-5]\n",
    "    \n",
    "    # Read in JSON file data\n",
    "    with open(json_file) as f:\n",
    "        json_file_data = json.load(f)\n",
    "    \n",
    "    for record in json_file_data:\n",
    "        reddit_comments.append({\n",
    "            'stock_symbol': stock_symbol,\n",
    "            'author': record['author'],\n",
    "            'created_utc': datetime.datetime.fromtimestamp(int(record['created_utc'])).isoformat(),\n",
    "            'id': record['id'],\n",
    "            'score': record['score'],\n",
    "            'body': record['body'],\n",
    "            'subreddit': record['subreddit'],\n",
    "            'sentiment': None,\n",
    "            'source': 'Custom Reddit API Query',\n",
    "            'post_id': re.findall(r'^\\/r\\/.+?\\/comments\\/(.+?)\\/.+$', json_file_data[0]['permalink'])[0]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1426bf21-be5e-4660-a500-d3dd6e20def6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a01aab0b534d77b57d53bf124b2f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve matching company comments from kaggle posts data\n",
    "@persist_to_file(os.path.join('all_data', 'comment_ids_without_author_info.json'))\n",
    "def get_comment_author(comment_id):\n",
    "    try:\n",
    "        return next(api.search_comments(ids=comment_id)).author\n",
    "    except StopIteration:\n",
    "        return None\n",
    "    \n",
    "companies_data = json.load(open('../dse203_final_project/companies.json'))\n",
    "kaggle_comments_df = pd.read_csv('kaggle_data/wsb-aug-2021-comments.csv')\n",
    "kaggle_comments_df['body'] = kaggle_comments_df['body'].fillna('')\n",
    "\n",
    "for stock_symbol in tqdm_notebook(companies_data):\n",
    "    selector = (\n",
    "        kaggle_comments_df['body'].str.match(construct_regex_filter(stock_symbol), flags=re.I)\n",
    "    )\n",
    "    for _, record in kaggle_comments_df[selector].iterrows():\n",
    "        reddit_comments.append({\n",
    "            'stock_symbol': stock_symbol,\n",
    "            'author': None,\n",
    "            'created_utc': datetime.datetime.fromtimestamp(int(record['created_utc'])).isoformat(),\n",
    "            'id': record['id'],\n",
    "            'score': record['score'],\n",
    "            'body': record['body'],\n",
    "            'subreddit': 'wallstreetbets',\n",
    "            'sentiment': record['sentiment'],\n",
    "            'source': 'Kaggle',\n",
    "            'post_id': re.findall(r'^\\/r\\/.+?\\/comments\\/(.+?)\\/.+$', json_file_data[0]['permalink'])[0],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d170b6b3-b4ae-4aa3-b2ea-cd1fffdf3a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e919da086c945808325e9467d66e9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3789903cd8fb487782cad1834f609546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Kaggle dataset authors\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = set()\n",
    "    comment_ids = [i['id'] for i in reddit_comments if i['source'] == 'Kaggle']\n",
    "    for comment_id in comment_ids:\n",
    "        futures.add(executor.submit(get_comment_author, comment_id))\n",
    "    \n",
    "    for future in tqdm_notebook(as_completed(futures), total=len(comment_ids)):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "# Add Kaggle dataset authors to reddit_comments\n",
    "for reddit_comment in tqdm_notebook([i for i in reddit_comments if (i['source'] == 'Kaggle') and (i['author'] is None)]):\n",
    "    reddit_comment['author'] = get_comment_author(reddit_comment['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbb6c8-f54d-40b2-bc2a-50ef2d97b080",
   "metadata": {},
   "source": [
    "# Write to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93e8071a-7275-485d-b879-5b02649948d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('all_data', 'reddit_submissions.json'), 'w') as f:\n",
    "    json.dump(reddit_posts, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f69b3576-1d0a-49e4-9729-39b725876aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('all_data', 'reddit_comments.json'), 'w') as f:\n",
    "    json.dump(reddit_comments, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
