{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9605a45-30a6-4a04-8392-fa1e4cfe9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939e2baf-0182-4eb9-aa37-fd7450f968fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_CLIENT_ID = '4FEO91ch16g-P_n8AxU2_A'\n",
    "REDDIT_CLIENT_SECRET = 'Q2cyR53G5a6IpsSAzLbevNtBiITYhw'\n",
    "REDDIT_USER_AGENT = 'desktop:DSE203 (by u/Life_is_Life)'\n",
    "\n",
    "SUBREDDITS_TO_EXTRACT = (\n",
    "    # Sources:\n",
    "    #  - https://thehiveindex.com/topics/investing/platform/reddit/\n",
    "    #  - https://www.investopedia.com/reddit-top-investing-and-trading-communities-5189322\n",
    "    'stocks',\n",
    "    'wallstreetbets',\n",
    "    'pennystocks',\n",
    "    'investing',\n",
    "    'Wallstreetbetsnew',\n",
    "    'StockMarket',\n",
    "    'options',\n",
    "    'RobinHood',\n",
    "    'RobinHoodPennyStocks',\n",
    "    'weedstocks',\n",
    "    'smallstreetbets',\n",
    "    'SecurityAnalysis',\n",
    "    'CanadianInvestor',\n",
    "    'SPACs',\n",
    "    'InvestmentClub',\n",
    "    'ValueInvesting',\n",
    "    'investing_discussion',\n",
    "    'stonks',\n",
    "    'shroomstocks',\n",
    ")\n",
    "\n",
    "ROOT_SAVE_DIR = 'top_reddit_posts_and_comments'\n",
    "JOB_DIR = str(int(pd.to_datetime('today').to_pydatetime().timestamp()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c075dbe8-81a3-4302-8def-e77d975f2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id = REDDIT_CLIENT_ID,\n",
    "    client_secret = REDDIT_CLIENT_SECRET,\n",
    "    user_agent = REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "api = PushshiftAPI(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a37e7c22-76d2-41f2-bc52-fbae569d56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2020-06-18'\n",
    "END_DATE = '2021-12-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944a6d49-52e4-4b9c-9a97-57e2ac421f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(ROOT_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(ROOT_SAVE_DIR, JOB_DIR), exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d023589c-b44a-4dc8-8f9a-204cf08b8812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n",
      "(1592463600, 1592549999)\n"
     ]
    }
   ],
   "source": [
    "# This object is a list. Each item in the list will have a 2-tuple (start_epoch, end_epoch)\n",
    "dates_to_extract = list()\n",
    "\n",
    "for date_to_extract in pd.date_range(START_DATE, END_DATE):\n",
    "    dates_to_extract.append(\n",
    "        tuple([\n",
    "            int(date_to_extract.to_pydatetime().timestamp()),\n",
    "            int((date_to_extract + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)).to_pydatetime().timestamp())\n",
    "        ])\n",
    "    )\n",
    "\n",
    "print(len(dates_to_extract))\n",
    "print(dates_to_extract[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d838529-c7ff-4643-94de-0d3f20f05cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0e193e2a784141b8af5383decec4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Development\\DSE203_FinalProject\\venv\\lib\\site-packages\\psaw\\PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "D:\\Development\\DSE203_FinalProject\\venv\\lib\\site-packages\\psaw\\PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    }
   ],
   "source": [
    "with tqdm_notebook(total=len(SUBREDDITS_TO_EXTRACT) * len(dates_to_extract)) as pbar:\n",
    "    for (start_epoch, end_epoch) in dates_to_extract[::-1]:\n",
    "        date_data_list = list()\n",
    "        \n",
    "        for subreddit_to_extract in SUBREDDITS_TO_EXTRACT:\n",
    "            submissions = list(api.search_submissions(\n",
    "                after = start_epoch,\n",
    "                before = end_epoch,\n",
    "                subreddit = subreddit_to_extract,\n",
    "                sort_type = 'score',\n",
    "                sort = 'desc',\n",
    "                limit = 25\n",
    "            ))\n",
    "            \n",
    "            for submission in submissions:\n",
    "                submission_id = submission.id\n",
    "            \n",
    "                date_data_list.append({\n",
    "                    'subreddit': subreddit_to_extract,\n",
    "                    'id': submission_id,\n",
    "                })\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update()\n",
    "        \n",
    "        save_file_name = str(dt.datetime.fromtimestamp(start_epoch).strftime('%Y%m%d')) + '.json'\n",
    "        with open(os.path.join(ROOT_SAVE_DIR, JOB_DIR, save_file_name), 'w') as f:\n",
    "            json.dump(date_data_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593519c9-d0a7-4746-b22e-aeb90640c44d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce97eaec-b3f6-4ea5-a096-e720c633c7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cac23-3a97-49c4-b340-f631e5c6930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm_notebook(total=len(SUBREDDITS_TO_EXTRACT) * len(dates_to_extract)) as pbar:\n",
    "    for (start_epoch, end_epoch) in dates_to_extract:\n",
    "        date_data_list = list()\n",
    "        \n",
    "        for subreddit_to_extract in SUBREDDITS_TO_EXTRACT:\n",
    "            submissions = list(api.search_submissions(\n",
    "                after = start_epoch,\n",
    "                before = end_epoch,\n",
    "                subreddit = subreddit_to_extract,\n",
    "                sort_type = 'score',\n",
    "                sort = 'desc',\n",
    "                limit = 25\n",
    "            ))\n",
    "            \n",
    "            for submission in submissions:\n",
    "                submission_id = submission.id\n",
    "                \n",
    "#                 reddit_submission = reddit.submission(id=submission_id)\n",
    "#                 submission_url = reddit_submission.url\n",
    "#                 submission_title = reddit_submission.title\n",
    "#                 submission_selftext = reddit_submission.selftext\n",
    "#                 submission_created = dt.datetime.fromtimestamp(reddit_submission.created).isoformat()\n",
    "                \n",
    "#                 submission_comments = []\n",
    "#                 for submission_comment in reddit_submission.comments:\n",
    "#                     if isinstance(submission_comment, praw.models.MoreComments):\n",
    "#                         continue\n",
    "                        \n",
    "#                     submission_comments.append({\n",
    "#                         'author': submission_comment.author.name if submission_comment.author is not None else None,\n",
    "#                         'created': dt.datetime.fromtimestamp(submission_comment.created).isoformat(),\n",
    "#                         'body': submission_comment.body,\n",
    "#                     })\n",
    "            \n",
    "                date_data_list.append({\n",
    "                    'id': submission_id,\n",
    "                    # 'url': submission_url,\n",
    "                    # 'title': submission_title,\n",
    "                    # 'selftext': submission_selftext,\n",
    "                    # 'created': submission_created,\n",
    "                    # 'comments': submission_comments,\n",
    "                })\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update()\n",
    "        \n",
    "        save_file_name = str(dt.datetime.fromtimestamp(start_epoch).strftime('%Y%m%d')) + '.json'\n",
    "        with open(os.path.join(ROOT_SAVE_DIR, JOB_DIR, save_file_name), 'w') as f:\n",
    "            json.dumps(date_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79380438-975d-4926-bb70-5c9cc386e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_to_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7139a-5467-4b4d-9082-a5bc9564c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.datetime.fromtimestamp(start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3b603-a9db-45ef-844e-374b5c77f7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = api.search_submissions(\n",
    "                after = start_epoch,\n",
    "                before = end_epoch,\n",
    "                subreddit = subreddit_to_extract,\n",
    "                sort_type = 'score',\n",
    "                sort = 'desc',\n",
    "                limit = 25\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac9bf3-42b4-41fa-a487-290ec7e37217",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c306c56-98f6-45e3-80e8-79303701afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reddit.submission(id='qkolqi').comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a84cab-5e7e-43fa-82b9-66dfc6e1fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.config.ratelimit_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89871b6a-182a-41f5-912a-cef9389b1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172c593-56ce-433b-a364-1c68432fa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad27be6-7e22-4384-a24d-dc9546d5965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6fe356-6171-4be9-a6a8-7167414787d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(reddit.submission(id='r9g7ks').comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094d200-ab09-4443-816a-f31a6e2f8804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
